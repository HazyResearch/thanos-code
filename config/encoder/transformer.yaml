module: transformer
d_model: null
learn_pos: True # has to be false for mixer - args.model is not "mixer",
n_heads: 8
head_dropout: 0.1
n_layers: 7
patch_size: 4
dropout: 0.05
use_cls_token: True
ret_cls_token: True # if True, only return embedding of CLS token
l_max: 65
_target_: null
path_to_checkpoint: null
source_module: null
